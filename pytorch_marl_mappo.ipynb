{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import TensorDictModule\n",
    "# 텐서 딕셔너리를 관리하고 모듈화 도움\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "# 정규 분포 매개변수 추출에 사용\n",
    "from torch import multiprocessing\n",
    "# 병렬 처리를 통해 데이터 수집 및 훈련 속도 높임\n",
    "\n",
    "# Data collection\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "# 동기화된 데이터 수집기 제공. 환경에서 데이터를 효율적으로 수집하는 데 사용\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "# 경험 재생 버퍼 구현\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "# 중복 없이 샘플을 추출하는 샘플러\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "# 텐서 저장소를 구현하여 데이터 메모리 사용을 효율화\n",
    "\n",
    "# Env\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "# rewardsum: 보상의 합계를 계산하는 데 사용되는 환경 변형 도구\n",
    "# transformedenv: 환경을 변형하여 강화학습에 맞는 형태로 변경\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "# 멀티 에이전트 시스템의 환경 라이브러리\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "# 환경의 스펙이 올바르게 설정되었는지 검증\n",
    "\n",
    "# Multi-agent network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "# multiagentmlp: 멀티 에이전트용 다층 퍼셉트론 정의\n",
    "# probabilisticactor: 확률적 행동 선택 담당\n",
    "# tanhnormal: 하이퍼볼릭 탄젠트를 활용한 정규 분포로 정책을 정의\n",
    "\n",
    "# Loss\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "# clipppoloss: ppo 알고리즘의 손실 함수\n",
    "# valueestimators: 가치 추정 모델 관리\n",
    "\n",
    "# Utils\n",
    "torch.manual_seed(0)\n",
    "# 결과의 재현성을 보장하기 위해 랜덤 시드 고정\n",
    "from matplotlib import pyplot as plt\n",
    "# 결과 시각화\n",
    "from tqdm import tqdm\n",
    "# 진행 상황을 시각적으로 보여주는 진행 바"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device  # The device where the simulator is run (VMAS can run on GPU)\n",
    "# 현재 멀티프로세싱 환경에서 fork가 사용되고 있는지 확인\n",
    "# fork는 일부 gpu 작업에서 비호환성을 유발할 수 있기 때문에 이를 감안하여 디바이스 설정\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 6_000  # Number of team frames collected per training iteration(반복)\n",
    "# 에이전트들이 환경에서 6000 프레임을 수행. 무슨 말이지\n",
    "n_iters = 10  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training\n",
    "num_epochs = 30  # Number of optimization steps per training iteration\n",
    "minibatch_size = 400  # Size of the mini-batches in each optimization step\n",
    "lr = 3e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
    "# 너무 큰 그래디언트를 방지하여 안정적인 학습을 도움\n",
    "\n",
    "# PPO\n",
    "clip_epsilon = 0.2  # clip value for PPO loss\n",
    "gamma = 0.99  # discount factor\n",
    "lmbda = 0.9  # lambda for generalised advantage estimation\n",
    "entropy_eps = 1e-4  # coefficient of the entropy term in the PPO loss\n",
    "# 엔트로피 보너스의 가중치를 조절하는 값. 에이전트의 행동 다양성을 높이기 위해 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Episode steps before done\n",
    "# 한 에피소드의 스텝 제한\n",
    "num_vmas_envs = (\n",
    "    frames_per_batch // max_steps\n",
    ")  # Number of vectorized envs. frames_per_batch should be divisible by this number\n",
    "# frames_per_batch, max_steps를 이용해서 병렬 환경 수를 계산\n",
    "# 지금은 6000//100 = 60개의 환경이 병렬로 돌아감\n",
    "scenario_name = \"navigation\"\n",
    "# vmas에서 제공하는 시뮬레이션 시나리오 중 하나 선택\n",
    "n_agents = 3\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=num_vmas_envs,\n",
    "    continuous_actions=True,  # VMAS supports both continuous and discrete actions\n",
    "    # 에이전트가 연속적인 행동을 할 수 있도록 설정. vmas는 연속 및 이산 행동을 모두 지원\n",
    "    max_steps=max_steps,\n",
    "    device=vmas_device,\n",
    "    # Scenario kwargs\n",
    "    n_agents=n_agents,  # These are custom kwargs that change for each VMAS scenario, see the VMAS repo to know more.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agents: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([60, 3, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([60, 3])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([60]))\n",
      "reward_spec: Composite(\n",
      "    agents: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([60, 3, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([60, 3])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([60]))\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([60, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([60]))\n",
      "observation_spec: Composite(\n",
      "    agents: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([60, 3, 18]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        info: Composite(\n",
      "            pos_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            final_rew: UnboundedContinuous(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            agent_collisions: UnboundedContinuous(\n",
      "                shape=torch.Size([60, 3, 1]),\n",
      "                space=ContinuousBox(\n",
      "                    low=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                    high=Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "                device=cuda:0,\n",
      "                dtype=torch.float32,\n",
      "                domain=continuous),\n",
      "            device=cuda:0,\n",
      "            shape=torch.Size([60, 3])),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([60, 3])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([60]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", env.full_action_spec)     # 행동 공간\n",
    "print(\"reward_spec:\", env.full_reward_spec)     # 보상 영역\n",
    "print(\"done_spec:\", env.full_done_spec)         # 완료 영역\n",
    "print(\"observation_spec:\", env.observation_spec)    # 환경으로부터 관찰할 수 있는 모든 출력의 범위\n",
    "\n",
    "# 아래의 출력이 뭘 뜻하는지는 아직 모르겠는데\n",
    "# 나중에 내가 환경을 세팅하면 이에 관해서 확인을 하고 넘어가야하지 않을까..\n",
    "# 환경의 설정이 적절한지 세팅값을 확인해보는 것 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", env.action_keys)\n",
    "print(\"reward_keys:\", env.reward_keys)\n",
    "print(\"done_keys:\", env.done_keys)\n",
    "\n",
    "# 액션, 보상을 에이전트 별로 구분하여 독립적으로 관리\n",
    "# 완료는 에피소드 공통으로 적용\n",
    "# tensordict 형식의 데이터를 독립적으로 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env,\n",
    "    RewardSum(in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]),\n",
    ")\n",
    "\n",
    "# 변환기능을 사용하여 환경의 입출력을 수정하는 기능\n",
    "# 여기서는 다중 에이전트 환경에서 보상을 합산하는 변환을 추가한 것\n",
    "# rewardsum: 보상 합상 변환. 에피소드 전체에 걸쳐 보상을 누적하여 기록\n",
    "# in keys: 에이전트 별 보상\n",
    "# out keys: 변환 결과 저장 위치, 에이전트의 하위 키인 에피소드 보상에 전체 보상 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 14:53:20,596 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)\n",
    "# 환경에서 샘플 데이터를 생성하여 정확히 작동하는지 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of three steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 5, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 5, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        info: TensorDict(\n",
      "                            fields={\n",
      "                                agent_collisions: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                                final_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                                pos_rew: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                            batch_size=torch.Size([60, 5, 3]),\n",
      "                            device=cuda:0,\n",
      "                            is_shared=True),\n",
      "                        observation: Tensor(shape=torch.Size([60, 5, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        reward: Tensor(shape=torch.Size([60, 5, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 5, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                done: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 5]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60, 5]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Shape of the rollout TensorDict: torch.Size([60, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(\"rollout of three steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n",
    "# 세팅 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_policy = True\n",
    "\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=2 * env.action_spec.shape[-1],  # 2 * n_actions_per_agents\n",
    "        n_agents=env.n_agents,\n",
    "        centralised=False,  # the policies are decentralised (ie each agent will act from its observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor(),  # this will just separate the last dimension into two outputs: a loc and a non-negative scale\n",
    ")\n",
    "# 정책 네트워크 설정\n",
    "# multiagentmlp: 다층 퍼셉트론 네트워크 설정\n",
    "# centralised: 정보를 공유할 건지 선택하는 건데. 아래의 비평가랑 무슨 차이가 있는거지\n",
    "# normalparamrxtractor: 네트워크 출력의 마지막 차원을 평균과 표준편차로 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    ")\n",
    "# 신경망을 감싸라는데\n",
    "# 입출력을 신경망에 연결시켜주는 거 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.unbatched_action_spec,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "        \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\"),\n",
    ")  # we'll need the log-prob for the PPO loss\n",
    "# 여기가 전체적인 정책을 정의하는 듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_parameters_critic = True\n",
    "mappo = True  # IPPO if False\n",
    "# mappo: 전체 관찰 가능성을 갖춘 중앙 비평가\n",
    "# ippo: 분산된 비평가. \n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,  # 1 value per agent\n",
    "    n_agents=env.n_agents,\n",
    "    centralised=mappo,\n",
    "    share_params=share_parameters_critic,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh,\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")\n",
    "\n",
    "# 위의 1,2단계랑 상당히 유사함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running policy: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                loc: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                sample_log_prob: Tensor(shape=torch.Size([60, 3]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                scale: Tensor(shape=torch.Size([60, 3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Running value: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                episode_reward: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                info: TensorDict(\n",
      "                    fields={\n",
      "                        agent_collisions: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        final_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        pos_rew: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([60, 3]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([60, 3, 18]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_value: Tensor(shape=torch.Size([60, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([60, 3]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([60, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([60]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Running policy:\", policy(env.reset()))\n",
    "print(\"Running value:\", critic(env.reset()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n",
    "# 에이전트 정책이 환경과 상호작용하여 데이터를 수집\n",
    "# 1. 환경 리셋\n",
    "# 2. 최신 관찰을 사용하여 정책을 통해 행동 계산\n",
    "# 3. 환경에서 스텝 실행\n",
    "# 4. 마지막 두 단계를 반복하며, 환경이 멈추라는 신호를 보내거나 done이 될 때까지 계속 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch, device=device\n",
    "    ),  # We store the frames_per_batch collected at each iteration\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size,  # We will sample minibatches of this size\n",
    ")\n",
    "# 학습에 쓴다는데?\n",
    "# batch size가 여기서 등장하네?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False,  # Important to avoid normalizing across the agent dimension\n",
    ")\n",
    "loss_module.set_keys(  # We have to tell the loss where to find the keys\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    sample_log_prob=(\"agents\", \"sample_log_prob\"),\n",
    "    value=(\"agents\", \"state_value\"),\n",
    "    # These last 2 keys will be expanded to match the reward shape\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")  # We build GAE\n",
    "GAE = loss_module.value_estimator\n",
    "\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode_reward_mean = 2.713294267654419: 100%|██████████| 10/10 [00:41<00:00,  4.18s/it]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=n_iters, desc=\"episode_reward_mean = 0\")\n",
    "# 학습 진행상황 시각화\n",
    "\n",
    "episode_reward_mean_list = []\n",
    "# 학습 중 평균 에피소드 보상 저장\n",
    "for tensordict_data in collector:\n",
    "    # 수집된 데이터를 반복적으로 처리\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"done\"),\n",
    "        tensordict_data.get((\"next\", \"done\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    tensordict_data.set(\n",
    "        (\"next\", \"agents\", \"terminated\"),\n",
    "        tensordict_data.get((\"next\", \"terminated\"))\n",
    "        .unsqueeze(-1)\n",
    "        .expand(tensordict_data.get_item_shape((\"next\", env.reward_key))),\n",
    "    )\n",
    "    # We need to expand the done and terminated to match the reward shape (this is expected by the value estimator)\n",
    "    # done, terminated의 데이터 형식을 reward랑 동일하게 맞춰서 처리가 가능하도록 함\n",
    "\n",
    "    with torch.no_grad():\n",
    "        GAE(\n",
    "            tensordict_data,\n",
    "            params=loss_module.critic_network_params,\n",
    "            target_params=loss_module.target_critic_network_params,\n",
    "        )  # Compute GAE and add it to the data\n",
    "    # GAE는 advantage 값을 계산하는 방법으로, 에이전트가 특정 행동을 했을 때 얼마나 좋은 결과를 얻을 수 있을지를 평가\n",
    "    # torch.no_grad: 학습이 아닌 데이터 계산임으로 그래디언트 추적 안함...왜?? 학습 아니었어..?\n",
    "\n",
    "    data_view = tensordict_data.reshape(-1)  # Flatten the batch size to shuffle data\n",
    "    replay_buffer.extend(data_view)\n",
    "    # 데이터를 평탄화 후, 리플레이 버퍼에 저장\n",
    "    # 평탄화는 데이터를 랜덤하게 섞고 샘플링하기 쉽게 만듦\n",
    "    # 데이터를 랜덤하게 왜 섞는데..?\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for _ in range(frames_per_batch // minibatch_size):\n",
    "            subdata = replay_buffer.sample()\n",
    "            loss_vals = loss_module(subdata)\n",
    "\n",
    "            loss_value = (\n",
    "                loss_vals[\"loss_objective\"]\n",
    "                + loss_vals[\"loss_critic\"]\n",
    "                + loss_vals[\"loss_entropy\"]\n",
    "            )\n",
    "\n",
    "            loss_value.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                loss_module.parameters(), max_grad_norm\n",
    "            )  # Optional\n",
    "\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "    # 데이터를 미니배치로 샘플링(replay buffer sample)한 후, 손실 계산\n",
    "\n",
    "    collector.update_policy_weights_()\n",
    "    # 정책 가중치 업데이트\n",
    "\n",
    "    # Logging\n",
    "    done = tensordict_data.get((\"next\", \"agents\", \"done\"))\n",
    "    episode_reward_mean = (\n",
    "        tensordict_data.get((\"next\", \"agents\", \"episode_reward\"))[done].mean().item()\n",
    "    )\n",
    "    # 에피소드가 종료된 상태에서 평균 에피소드 보상을 계산하고 저장\n",
    "    episode_reward_mean_list.append(episode_reward_mean)\n",
    "    pbar.set_description(f\"episode_reward_mean = {episode_reward_mean}\", refresh=False)\n",
    "    pbar.update()\n",
    "    # 진행률 표시줄에 업데이트된 평균 보상 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode_reward_mean_list)\n",
    "plt.xlabel(\"Training iterations\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode reward mean\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrendering_callback\u001b[39m(env, td):\n\u001b[1;32m      7\u001b[0m     env\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(Image\u001b[38;5;241m.\u001b[39mfromarray(env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 8\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     10\u001b[0m    env\u001b[38;5;241m.\u001b[39mrollout(\n\u001b[1;32m     11\u001b[0m        max_steps\u001b[38;5;241m=\u001b[39mmax_steps,\n\u001b[1;32m     12\u001b[0m        policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m        break_when_any_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m    )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "import pyvirtualdisplay\n",
    "display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
    "display.start()\n",
    "from PIL import Image\n",
    "\n",
    "def rendering_callback(env, td):\n",
    "    env.frames.append(Image.fromarray(env.render(mode=\"rgb_array\")))\n",
    "env.frames = []\n",
    "with torch.no_grad():\n",
    "   env.rollout(\n",
    "       max_steps=max_steps,\n",
    "       policy=policy,\n",
    "       callback=rendering_callback,\n",
    "       auto_cast_to_device=True,\n",
    "       break_when_any_done=False,\n",
    "   )\n",
    "env.frames[0].save(\n",
    "    f\"{scenario_name}.gif\",\n",
    "    save_all=True,\n",
    "    append_images=env.frames[1:],\n",
    "   duration=3,\n",
    "   loop=0,\n",
    ")\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(open(f\"{scenario_name}.gif\", \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# with torch.no_grad():\n",
    "#    env.rollout(\n",
    "#        max_steps=max_steps,\n",
    "#        policy=policy,\n",
    "#        callback=lambda env, _: env.render(),\n",
    "#        auto_cast_to_device=True,\n",
    "#        break_when_any_done=False,\n",
    "#    )\n",
    "   \n",
    "# 주피터에서만 발생하는 문제가 생김\n",
    "# 한번 실행 후에 창이 사라지지 않음.\n",
    "# 먹통이 됨\n",
    "# 처음부터 실행시키면 랜더링은 되는데, 에피소드가 끝난 후 창이 종료되지 않음.\n",
    "# 뭐가 문제인지는 잘 모르겠고요\n",
    "\n",
    "# 가상화 방법으로 실행을 시키니 아주 잘 나온다.\n",
    "# 위의 방법을 사용 하자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
